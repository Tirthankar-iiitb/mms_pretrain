{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e973181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_trainer: https://github.com/huggingface/transformers/blob/main/examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829137d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-07 17:18:45.039064: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-07 17:18:48.314901: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os, math\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from datasets import DatasetDict, concatenate_datasets\n",
    "import datasets\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    SchedulerType,\n",
    "    Wav2Vec2Config,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2ForPreTraining,\n",
    "    get_scheduler,\n",
    "    #is_wandb_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import _compute_mask_indices, _sample_negative_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d6a482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "SAMPLINGRATE=16000\n",
    "SEED=25\n",
    "#WANDB_PROJECT='pretrain_wav2vec2_accelerator'\n",
    "#MODEL_NAME='patrickvonplaten/wav2vec2-base-v2'\n",
    "MODEL_NAME=\"patrickvonplaten/mms-300m\"\n",
    "#MODEL_NAME='facebook/mms-300m'\n",
    "#MODEL_NAME=\"facebook/wav2vec2-xls-r-300m\"\n",
    "DATASET='/root/openstream/kashmiri/pretrain/pretrain_dataset'\n",
    "#validation_split_percentage=10\n",
    "max_duration_in_seconds=25\n",
    "min_duration_in_seconds=2\n",
    "audio_column_name='audio'\n",
    "preprocessing_num_workers=1  #4\n",
    "cachedir='/root/openstream/kashmiri/pretrain/cache_dir'\n",
    "train_cache_file_name=f'{cachedir}/train_cache.cache'\n",
    "validation_cache_file_name=f'{cachedir}/valid_cache.cache'\n",
    "pad_to_multiple_of=None\n",
    "learning_rate=.005  #5e-5\n",
    "gradient_accumulation_steps=8\n",
    "gradient_checkpointing=True\n",
    "mask_time_prob=None\n",
    "mask_time_length=None\n",
    "pad_to_multiple_of=None\n",
    "per_device_train_batch_size=8\n",
    "per_device_eval_batch_size=8\n",
    "max_train_steps=20000\n",
    "num_warmup_steps=32000\n",
    "lr_scheduler_type='linear' \n",
    "# [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\", \"constant\", \"constant_with_warmup\"] \n",
    "\n",
    "gumbel_temperature_decay=0.999995\n",
    "min_gumbel_temperature=0.5\n",
    "max_gumbel_temperature=2.0\n",
    "\n",
    "num_train_epochs=1\n",
    "logging_steps=500\n",
    "saving_steps=10000\n",
    "\n",
    "\n",
    "output_dir='/root/openstream/kashmiri/pretrain/models/mms_pretrained_kashmiri'\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c94dfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForWav2Vec2Pretraining:\n",
    "    model: Wav2Vec2ForPreTraining\n",
    "    feature_extractor: Wav2Vec2FeatureExtractor\n",
    "    padding: Union[bool, str] = \"longest\"\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    mask_time_prob: Optional[float] = 0.65\n",
    "    mask_time_length: Optional[int] = 10\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # reformat list to dict and set to pytorch format\n",
    "        inp_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        batch = self.feature_extractor.pad(\n",
    "            inp_features,\n",
    "            padding=self.padding,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        device = batch[\"input_values\"].device\n",
    "        batch_size = batch[\"input_values\"].shape[0]\n",
    "\n",
    "        mask_indices_seq_length = self.model._get_feat_extract_output_lengths(batch[\"input_values\"].shape[-1])\n",
    "        # make sure masked sequence length is a Python scalar\n",
    "        mask_indices_seq_length = int(mask_indices_seq_length)\n",
    "\n",
    "        # make sure that no loss is computed on padded inputs\n",
    "        if batch.get(\"attention_mask\") is not None:\n",
    "            # compute real output lengths according to convolution formula\n",
    "            batch[\"sub_attention_mask\"] = self.model._get_feature_vector_attention_mask(\n",
    "                mask_indices_seq_length, batch[\"attention_mask\"]\n",
    "            )\n",
    "\n",
    "        features_shape = (batch_size, mask_indices_seq_length)\n",
    "\n",
    "        # sample randomly masked indices\n",
    "        mask_time_indices = _compute_mask_indices(\n",
    "            features_shape,\n",
    "            self.mask_time_prob,\n",
    "            self.mask_time_length,\n",
    "            attention_mask=batch.get(\"sub_attention_mask\"),\n",
    "        )\n",
    "\n",
    "        # sample negative indices\n",
    "        sampled_negative_indices = _sample_negative_indices(\n",
    "            features_shape,\n",
    "            self.model.config.num_negatives,\n",
    "            mask_time_indices=mask_time_indices,\n",
    "        )\n",
    "        batch[\"mask_time_indices\"] = torch.tensor(mask_time_indices, dtype=torch.long, device=device)\n",
    "        batch[\"sampled_negative_indices\"] = torch.tensor(sampled_negative_indices, dtype=torch.long, device=device)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe5e0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_grads(params, c):\n",
    "    \"\"\"Multiplies grads by a constant *c*.\"\"\"\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            if torch.is_tensor(c):\n",
    "                c = c.to(p.grad.device)\n",
    "            p.grad.data.mul_(c)\n",
    "\n",
    "\n",
    "def get_grad_norm(params, scale=1):\n",
    "    \"\"\"Compute grad norm given a gradient scale.\"\"\"\n",
    "    total_norm = 0.0\n",
    "    for p in params:\n",
    "        if p.grad is not None:\n",
    "            param_norm = (p.grad.detach().data / scale).norm(2)\n",
    "            total_norm += param_norm.item() ** 2\n",
    "    total_norm = total_norm**0.5\n",
    "    return total_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a62136cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the accelerator. We will let the accelerator handle device placement for us in this example.\n",
    "accelerator = Accelerator()\n",
    "\n",
    "logger.info(accelerator.state, main_process_only=False)\n",
    "\n",
    "datasets.utils.logging.set_verbosity_warning()\n",
    "transformers.utils.logging.set_verbosity_info()\n",
    "\n",
    "# set up weights and biases if available\n",
    "# if is_wandb_available():\n",
    "#     import wandb\n",
    "#     wandb.init(project=WANDB_PROJECT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f80a8ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--patrickvonplaten--mms-300m/snapshots/4ee317ce793c53dbc041fc4376c7558292dd38dc/preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_NAME)\n",
    "feature_extractor.do_normalize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66b541cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dset=DatasetDict().load_from_disk(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "933fd6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'dur'],\n",
       "        num_rows: 7575\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['audio', 'dur'],\n",
       "        num_rows: 841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cab01bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = int(max_duration_in_seconds * feature_extractor.sampling_rate)\n",
    "min_length = int(min_duration_in_seconds * feature_extractor.sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43762609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(batch):\n",
    "    sample = batch[audio_column_name]\n",
    "\n",
    "    inputs = feature_extractor(\n",
    "        sample[\"array\"], sampling_rate=sample[\"sampling_rate\"], max_length=max_length, truncation=True\n",
    "        #sample, sampling_rate=SAMPLINGRATE, max_length=max_length, truncation=True, padding=True\n",
    "    )\n",
    "    batch[\"input_values\"] = inputs.input_values[0]\n",
    "    batch[\"input_length\"] = len(inputs.input_values[0])\n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a03de55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/openstream/kashmiri/pretrain/pretrain_dataset/train/cache-810b03b772a56c4c.arrow\n",
      "Loading cached processed dataset at /root/openstream/kashmiri/pretrain/pretrain_dataset/val/cache-4405e6582e3a2a32.arrow\n"
     ]
    }
   ],
   "source": [
    "cache_file_names=None\n",
    "if train_cache_file_name is not None:\n",
    "    cache_file_names = {\"train\": train_cache_file_name, \"val\": validation_cache_file_name}\n",
    "    \n",
    "with accelerator.main_process_first():\n",
    "    vectorized_datasets = dset.map(\n",
    "        prepare_dataset,\n",
    "        num_proc=preprocessing_num_workers,\n",
    "        remove_columns=dset[\"train\"].column_names,\n",
    "        #cache_file_names=cache_file_names,\n",
    "    )\n",
    "\n",
    "#     if min_length > 0.0:\n",
    "#         vectorized_datasets = vectorized_datasets.filter(\n",
    "#             lambda x: x > min_length,\n",
    "#             num_proc=preprocessing_num_workers,\n",
    "#             input_columns=[\"input_length\"],\n",
    "#         )\n",
    "\n",
    "    vectorized_datasets = vectorized_datasets.remove_columns(\"input_length\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1825a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 7575\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['input_values'],\n",
       "        num_rows: 841\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21687175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "375b6385",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--patrickvonplaten--mms-300m/snapshots/4ee317ce793c53dbc041fc4376c7558292dd38dc/config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_prob\": 0.075,\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.28.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = Wav2Vec2Config.from_pretrained(MODEL_NAME)\n",
    "model = Wav2Vec2ForPreTraining(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a528b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/torchenv/lib/python3.8/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "\n",
    "mask_time_prob = config.mask_time_prob if mask_time_prob is None else mask_time_prob\n",
    "mask_time_length = config.mask_time_length if mask_time_length is None else mask_time_length\n",
    "\n",
    "data_collator = DataCollatorForWav2Vec2Pretraining(\n",
    "    model=model,\n",
    "    feature_extractor=feature_extractor,\n",
    "    \n",
    "    pad_to_multiple_of=pad_to_multiple_of,\n",
    "    mask_time_prob=mask_time_prob,\n",
    "    mask_time_length=mask_time_length,\n",
    ")\n",
    "train_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"train\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=per_device_train_batch_size,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    vectorized_datasets[\"val\"], collate_fn=data_collator, batch_size=per_device_eval_batch_size\n",
    ")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(\n",
    "    list(model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    betas=[0.9, 0.999],\n",
    "    eps=1e-8,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13138d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dd=next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7908bbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88cb35b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "947"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73aa04ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 169, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_train_steps,num_train_epochs,accelerator.num_processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e012c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd8420074c424b0f98e21eef17441eb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.virtualenvs/torchenv/lib/python3.8/site-packages/torch/utils/checkpoint.py:391: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# divide loss by gradient accumulation steps since gradients\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# are accumulated for multiple backward passes in PyTorch\u001b[39;00m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;241m1\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     36\u001b[0m accelerator\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# make sure that `num_losses` is summed for distributed training\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# and average gradients over losses of all devices\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train\n",
    "total_batch_size = per_device_train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "\n",
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(f\"  Num examples = {len(vectorized_datasets['train'])}\")\n",
    "logger.info(f\"  Num Epochs = {num_train_epochs}\")\n",
    "logger.info(f\"  Instantaneous batch size per device = {per_device_train_batch_size}\")\n",
    "logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # compute num of losses\n",
    "        \n",
    "        num_losses = batch[\"mask_time_indices\"].sum()\n",
    "        sub_attention_mask = batch.pop(\"sub_attention_mask\", None)\n",
    "        sub_attention_mask = (\n",
    "            sub_attention_mask if sub_attention_mask is not None else torch.ones_like(batch[\"mask_time_indices\"])\n",
    "        )\n",
    "        percent_masked = num_losses / sub_attention_mask.sum()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        # divide loss by gradient accumulation steps since gradients\n",
    "        # are accumulated for multiple backward passes in PyTorch\n",
    "        loss = outputs.loss / gradient_accumulation_steps\n",
    "        assert 1==2\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        # make sure that `num_losses` is summed for distributed training\n",
    "        # and average gradients over losses of all devices\n",
    "        if accelerator.state.num_processes > 1:\n",
    "            num_losses = accelerator.gather_for_metrics(num_losses).sum()\n",
    "            gradient_multiplier = accelerator.state.num_processes / num_losses\n",
    "            multiply_grads(model.module.parameters(), gradient_multiplier)\n",
    "        else:\n",
    "            multiply_grads(model.parameters(), 1 / num_losses)\n",
    "\n",
    "        # update step\n",
    "        if (step + 1) % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            # compute grad norm for monitoring\n",
    "            scale = (\n",
    "                accelerator.scaler._scale.item()\n",
    "                if hasattr(accelerator, \"scaler\") and accelerator.scaler is not None\n",
    "                else 1\n",
    "            )\n",
    "            if accelerator.state.num_processes > 1:\n",
    "                grad_norm = get_grad_norm(model.module.parameters(), scale)\n",
    "            else:\n",
    "                grad_norm = get_grad_norm(model.parameters(), scale)\n",
    "\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if not accelerator.optimizer_step_was_skipped:\n",
    "                lr_scheduler.step()\n",
    "            elif accelerator.is_local_main_process:\n",
    "                progress_bar.write(\n",
    "                    f\"Gradients have overflown - skipping update step... Updating gradient scale to {scale}...\"\n",
    "                )\n",
    "\n",
    "            # update gumbel temperature\n",
    "            gumbel_temperature = max(\n",
    "                max_gumbel_temperature * gumbel_temperature_decay**completed_steps,\n",
    "                min_gumbel_temperature,\n",
    "            )\n",
    "            if hasattr(model, \"module\"):\n",
    "                model.module.set_gumbel_temperature(gumbel_temperature)\n",
    "            else:\n",
    "                model.set_gumbel_temperature(gumbel_temperature)\n",
    "\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "        # logs\n",
    "        if (step + 1) % (gradient_accumulation_steps * logging_steps) == 0:\n",
    "            loss.detach()\n",
    "            outputs.contrastive_loss.detach()\n",
    "            outputs.diversity_loss.detach()\n",
    "\n",
    "            if accelerator.state.num_processes > 1:\n",
    "                loss = accelerator.gather_for_metrics(loss).sum()\n",
    "                outputs.contrastive_loss = accelerator.gather_for_metrics(outputs.contrastive_loss).sum()\n",
    "                outputs.diversity_loss = accelerator.gather_for_metrics(outputs.diversity_loss).sum()\n",
    "                percent_masked = accelerator.gather_for_metrics(percent_masked).sum()\n",
    "\n",
    "            train_logs = {\n",
    "                \"loss\": (loss * gradient_accumulation_steps) / num_losses,\n",
    "                \"constrast_loss\": outputs.contrastive_loss / num_losses,\n",
    "                \"div_loss\": outputs.diversity_loss / num_losses,\n",
    "                \"%_mask_idx\": percent_masked / accelerator.num_processes,\n",
    "                \"ppl\": outputs.codevector_perplexity,\n",
    "                \"lr\": torch.tensor(optimizer.param_groups[0][\"lr\"]),\n",
    "                \"temp\": torch.tensor(gumbel_temperature),\n",
    "                \"grad_norm\": torch.tensor(grad_norm),\n",
    "            }\n",
    "            log_str = \"\"\n",
    "            for k, v in train_logs.items():\n",
    "                log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
    "\n",
    "            if accelerator.is_local_main_process:\n",
    "                progress_bar.write(log_str)\n",
    "#                 if is_wandb_available():\n",
    "#                     wandb.log(train_logs)\n",
    "                    \n",
    "        # save model every `args.saving_steps` steps\n",
    "        if (step + 1) % (gradient_accumulation_steps * saving_steps) == 0:\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(\n",
    "                output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "            )\n",
    "        \n",
    "        # if completed steps > `args.max_train_steps` stop\n",
    "        if completed_steps >= max_train_steps:\n",
    "            break\n",
    "            \n",
    "    #validate\n",
    "    model.eval()\n",
    "    val_logs = {\n",
    "        \"val_loss\": 0,\n",
    "        \"val_contrastive_loss\": 0,\n",
    "        \"val_diversity_loss\": 0,\n",
    "        \"val_num_losses\": 0,\n",
    "        }\n",
    "    \n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            batch.pop(\"sub_attention_mask\", None)\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        val_logs[\"val_loss\"] += outputs.loss\n",
    "        val_logs[\"val_contrastive_loss\"] += outputs.contrastive_loss\n",
    "        val_logs[\"val_diversity_loss\"] += outputs.diversity_loss\n",
    "        val_logs[\"val_num_losses\"] += batch[\"mask_time_indices\"].sum()\n",
    "\n",
    "    # sum over devices in multi-processing\n",
    "    if accelerator.num_processes > 1:\n",
    "        val_logs = {k: accelerator.gather_for_metrics(v).sum() for k, v in val_logs.items()}\n",
    "\n",
    "    val_logs = {k: v / val_logs[\"val_num_losses\"] for k, v in val_logs.items()}\n",
    "\n",
    "    log_str = \"\"\n",
    "    for k, v in val_logs.items():\n",
    "        log_str += \"| {}: {:.3e}\".format(k, v.item())\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        progress_bar.write(log_str)\n",
    "#         if is_wandb_available():\n",
    "#             wandb.log(val_logs)\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d5734a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "handle_0 INTERNAL ASSERT FAILED at \"../c10/cuda/driver_api.cpp\":15, please report a bug to PyTorch. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torchenv/lib/python3.8/site-packages/accelerate/accelerator.py:1821\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1821\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torchenv/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.virtualenvs/torchenv/lib/python3.8/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: handle_0 INTERNAL ASSERT FAILED at \"../c10/cuda/driver_api.cpp\":15, please report a bug to PyTorch. "
     ]
    }
   ],
   "source": [
    "accelerator.backward(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60794cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
